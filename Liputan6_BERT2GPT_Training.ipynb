{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3192dd37",
   "metadata": {},
   "source": [
    "# Liputan6 ‚Äî BERT2GPT Indonesian Summarization Fine-tuning\n",
    "\n",
    "## üéØ Tujuan\n",
    "Fine-tuning model **cahya/bert2gpt-indonesian-summarization** dari Hugging Face untuk text summarization Bahasa Indonesia menggunakan dataset Liputan6.\n",
    "\n",
    "## üìö Model Information\n",
    "- **Model**: BERT-to-GPT2 Encoder-Decoder Architecture\n",
    "- **Pretrained**: cahya/bert2gpt-indonesian-summarization\n",
    "- **Source**: https://huggingface.co/cahya/bert2gpt-indonesian-summarization\n",
    "- **Language**: Indonesian (Bahasa Indonesia)\n",
    "- **Task**: Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791589cf",
   "metadata": {},
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "1. [Setup & Dependencies](#setup)\n",
    "2. [Load Preprocessed Data](#load-data)\n",
    "3. [Load BERT2GPT Model](#load-model)\n",
    "4. [Prepare Dataset for Training](#prepare-dataset)\n",
    "5. [Training Configuration](#training-config)\n",
    "6. [Fine-tuning](#fine-tuning)\n",
    "7. [Evaluation](#evaluation)\n",
    "8. [Inference & Testing](#inference)\n",
    "9. [Save Model](#save-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330435e",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup & Dependencies\n",
    "\n",
    "Install dan import library yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "972d3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# Uncomment jika belum terinstall\n",
    "# !pip install -q transformers datasets torch sentencepiece accelerate evaluate rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc4ec17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì transformers already installed\n",
      "‚úì evaluate already installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Auto-install transformers jika belum ada\n",
    "try:\n",
    "    from transformers import BertTokenizer, EncoderDecoderModel\n",
    "    print(\"‚úì transformers already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing transformers...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"torch\", \"sentencepiece\", \"accelerate\"])\n",
    "    from transformers import BertTokenizer, EncoderDecoderModel\n",
    "    print(\"‚úì transformers installed successfully\")\n",
    "\n",
    "# Install evaluate and rouge_score for metrics\n",
    "try:\n",
    "    import evaluate\n",
    "    print(\"‚úì evaluate already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing evaluate...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"evaluate\", \"rouge-score\"])\n",
    "    import evaluate\n",
    "    print(\"‚úì evaluate installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b42b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyTorch version: 2.9.0+cpu\n",
      "‚úì CUDA available: False\n",
      "\n",
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Evaluation\n",
    "import evaluate\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90150ddf",
   "metadata": {},
   "source": [
    "<a id=\"load-data\"></a>\n",
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Load data yang sudah di-preprocessing dari notebook sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27561dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Preprocessed data directory: output\\preprocessed\n",
      "üìÅ Output directory: output\\bert2gpt_finetuned\n",
      "\n",
      "‚öôÔ∏è  Preprocessing Config:\n",
      "  train_size: 9694\n",
      "  val_size: 549\n",
      "  test_size: 549\n",
      "  tokenizer_name: indolem/indobert-base-uncased\n",
      "  use_sample: True\n",
      "  preprocessing_mode: BERT2GPT-only\n",
      "  avg_article_tokens: 198\n",
      "  avg_summary_tokens: 27\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PREPROCESS_DIR = Path(\"./output/preprocessed\")\n",
    "OUTPUT_DIR = Path(\"./output/bert2gpt_finetuned\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Preprocessed data directory: {PREPROCESS_DIR}\")\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load config\n",
    "with open(PREPROCESS_DIR / \"config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  Preprocessing Config:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19034552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading preprocessed BERT data...\n",
      "\n",
      "‚úì Loaded from bert_data.pkl (optimized BERT format)\n",
      "\n",
      "‚úì Train set: 9,694 samples\n",
      "‚úì Validation set: 549 samples\n",
      "‚úì Test set: 549 samples\n",
      "‚úì Total: 10,792 samples\n",
      "\n",
      "üìã Sample from training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_article</th>\n",
       "      <th>clean_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liputan6 . com , Pandeglang : Sebuah ledakan k...</td>\n",
       "      <td>Dua orang tewas seketika akibat ledakan dahsya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liputan6 . com , Ottawa : Setelah keputusan De...</td>\n",
       "      <td>Kanada menyetujui tindakan DK PBB dan akan iku...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_article  \\\n",
       "0  Liputan6 . com , Pandeglang : Sebuah ledakan k...   \n",
       "1  Liputan6 . com , Ottawa : Setelah keputusan De...   \n",
       "\n",
       "                                       clean_summary  \n",
       "0  Dua orang tewas seketika akibat ledakan dahsya...  \n",
       "1  Kanada menyetujui tindakan DK PBB dan akan iku...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load preprocessed data (BERT format)\n",
    "print(\"üìÇ Loading preprocessed BERT data...\\n\")\n",
    "\n",
    "# Try to load from bert_data.pkl (optimized format)\n",
    "try:\n",
    "    with open(PREPROCESS_DIR / \"bert_data.pkl\", 'rb') as f:\n",
    "        bert_data = pickle.load(f)\n",
    "    \n",
    "    print(\"‚úì Loaded from bert_data.pkl (optimized BERT format)\")\n",
    "    \n",
    "    # Convert to dataframes\n",
    "    df_train = pd.DataFrame({\n",
    "        'clean_article': bert_data['train']['articles'],\n",
    "        'clean_summary': bert_data['train']['summaries']\n",
    "    })\n",
    "    df_val = pd.DataFrame({\n",
    "        'clean_article': bert_data['val']['articles'],\n",
    "        'clean_summary': bert_data['val']['summaries']\n",
    "    })\n",
    "    df_test = pd.DataFrame({\n",
    "        'clean_article': bert_data['test']['articles'],\n",
    "        'clean_summary': bert_data['test']['summaries']\n",
    "    })\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  bert_data.pkl not found, loading from CSV files...\")\n",
    "    df_train = pd.read_csv(PREPROCESS_DIR / \"train.csv\")\n",
    "    df_val = pd.read_csv(PREPROCESS_DIR / \"val.csv\")\n",
    "    df_test = pd.read_csv(PREPROCESS_DIR / \"test.csv\")\n",
    "    print(\"‚úì Loaded from CSV files\")\n",
    "\n",
    "print(f\"\\n‚úì Train set: {len(df_train):,} samples\")\n",
    "print(f\"‚úì Validation set: {len(df_val):,} samples\")\n",
    "print(f\"‚úì Test set: {len(df_test):,} samples\")\n",
    "print(f\"‚úì Total: {len(df_train) + len(df_val) + len(df_test):,} samples\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìã Sample from training data:\")\n",
    "display(df_train[['clean_article', 'clean_summary']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a03914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Using FULL dataset for training\n"
     ]
    }
   ],
   "source": [
    "# Optional: Reduce dataset size for faster training (development mode)\n",
    "USE_SAMPLE = False  # Set to True untuk menggunakan subset kecil untuk testing\n",
    "SAMPLE_SIZE = 1000   # Jumlah sample untuk development\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    print(f\"‚ö° Using sample data ({SAMPLE_SIZE} samples per split) for faster development\\n\")\n",
    "    df_train = df_train.sample(n=min(SAMPLE_SIZE, len(df_train)), random_state=SEED).reset_index(drop=True)\n",
    "    df_val = df_val.sample(n=min(SAMPLE_SIZE//5, len(df_val)), random_state=SEED).reset_index(drop=True)\n",
    "    df_test = df_test.sample(n=min(SAMPLE_SIZE//5, len(df_test)), random_state=SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úì Train: {len(df_train)} samples\")\n",
    "    print(f\"‚úì Val: {len(df_val)} samples\")\n",
    "    print(f\"‚úì Test: {len(df_test)} samples\")\n",
    "else:\n",
    "    print(\"üìä Using FULL dataset for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5738331",
   "metadata": {},
   "source": [
    "<a id=\"load-model\"></a>\n",
    "## 3. Load BERT2GPT Model\n",
    "\n",
    "Load pre-trained model **cahya/bert2gpt-indonesian-summarization** dari Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f34e1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading model: cahya/bert2gpt-indonesian-summarization\n",
      "\n",
      "üîÑ Loading BertTokenizer...\n",
      "‚ÑπÔ∏è  Using IndoBERT tokenizer (compatible with BERT2GPT)\n",
      "\n",
      "‚úì Tokenizer loaded from IndoBERT vocab\n",
      "\n",
      "‚úì Tokenizer configured:\n",
      "  Vocab size: 30521\n",
      "  BOS token: [CLS] (ID: 2)\n",
      "  EOS token: [SEP] (ID: 3)\n",
      "  PAD token: [PAD] (ID: 0)\n",
      "\n",
      "üîÑ Loading BERT2GPT model...\n",
      "‚úì Tokenizer loaded from IndoBERT vocab\n",
      "\n",
      "‚úì Tokenizer configured:\n",
      "  Vocab size: 30521\n",
      "  BOS token: [CLS] (ID: 2)\n",
      "  EOS token: [SEP] (ID: 3)\n",
      "  PAD token: [PAD] (ID: 0)\n",
      "\n",
      "üîÑ Loading BERT2GPT model...\n",
      "‚ö†Ô∏è  Error loading model: Can't load the model for 'cahya/bert2gpt-indonesian-summarization'. If you were trying to load it fr\n",
      "\n",
      "üîÑ Trying alternative loading method...\n",
      "‚ö†Ô∏è  Error loading model: Can't load the model for 'cahya/bert2gpt-indonesian-summarization'. If you were trying to load it fr\n",
      "\n",
      "üîÑ Trying alternative loading method...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b6cb6c87bb4302a275e81e7e6967e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded from downloaded snapshot\n",
      "\n",
      "‚úì Model loaded successfully\n",
      "  Encoder: bert\n",
      "  Decoder: gpt2\n",
      "  Total parameters: 263,424,000\n",
      "\n",
      "‚úì Model moved to: cpu\n"
     ]
    }
   ],
   "source": [
    "# Model checkpoint\n",
    "MODEL_CHECKPOINT = \"cahya/bert2gpt-indonesian-summarization\"\n",
    "\n",
    "print(f\"ü§ñ Loading model: {MODEL_CHECKPOINT}\\n\")\n",
    "\n",
    "# Workaround for transformers 4.57+ checking optional chat_templates directory\n",
    "# Suppress warnings about missing optional files\n",
    "import logging\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Temporarily reduce logging level\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"üîÑ Loading BertTokenizer...\")\n",
    "print(\"‚ÑπÔ∏è  Using IndoBERT tokenizer (compatible with BERT2GPT)\\n\")\n",
    "\n",
    "# Download only essential tokenizer files (skip optional chat_templates)\n",
    "try:\n",
    "    # Download vocab file\n",
    "    vocab_file = hf_hub_download(\n",
    "        repo_id=\"indobenchmark/indobert-base-p1\",\n",
    "        filename=\"vocab.txt\"\n",
    "    )\n",
    "    \n",
    "    # Load from downloaded files\n",
    "    tokenizer = BertTokenizer(vocab_file=vocab_file)\n",
    "    print(\"‚úì Tokenizer loaded from IndoBERT vocab\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Fallback: Using BERT-base multilingual\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    print(\"‚úì Tokenizer loaded successfully\")\n",
    "\n",
    "# Set special tokens (as per official documentation)\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "print(\"\\n‚úì Tokenizer configured:\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Load BERT2GPT model with error handling\n",
    "print(\"\\nüîÑ Loading BERT2GPT model...\")\n",
    "try:\n",
    "    # Try loading with ignore_mismatched_sizes\n",
    "    model = EncoderDecoderModel.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    print(\"‚úì Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error loading model: {str(e)[:100]}\")\n",
    "    print(\"\\nüîÑ Trying alternative loading method...\")\n",
    "    \n",
    "    # Download model files individually\n",
    "    try:\n",
    "        from huggingface_hub import snapshot_download\n",
    "        cache_dir = snapshot_download(\n",
    "            repo_id=MODEL_CHECKPOINT,\n",
    "            allow_patterns=[\"*.bin\", \"*.json\", \"*.txt\"],\n",
    "            ignore_patterns=[\"additional_chat_templates/*\"]\n",
    "        )\n",
    "        model = EncoderDecoderModel.from_pretrained(cache_dir)\n",
    "        print(\"‚úì Model loaded from downloaded snapshot\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Could not load model: {str(e2)[:100]}\")\n",
    "        raise\n",
    "\n",
    "# Restore logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.WARNING)\n",
    "\n",
    "# Set special tokens for generation\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"\\n‚úì Model loaded successfully\")\n",
    "print(f\"  Encoder: {model.config.encoder.model_type}\")\n",
    "print(f\"  Decoder: {model.config.decoder.model_type}\")\n",
    "print(f\"  Total parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"\\n‚úì Model moved to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565cba6",
   "metadata": {},
   "source": [
    "<a id=\"prepare-dataset\"></a>\n",
    "## 4. Prepare Dataset for Training\n",
    "\n",
    "Konversi dataframe menjadi Hugging Face Dataset dan tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "488fe58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting to Hugging Face Dataset format...\n",
      "\n",
      "‚úì Dataset created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'summary'],\n",
      "        num_rows: 9694\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'summary'],\n",
      "        num_rows: 549\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'summary'],\n",
      "        num_rows: 549\n",
      "    })\n",
      "})\n",
      "\n",
      "üìã Sample data:\n",
      "{'article': 'Liputan6 . com , Pandeglang : Sebuah ledakan keras terjadi di Kampung Ciruang , Desa Pejamben , Kecamatan Carita , Pandeglang , Banten , Selasa ( 1/7 ) , sekitar pukul 13 . 30 WIB . Pusat ledakan di sebuah gubuk di kampung yang berjarak satu kilometer dari tempat rekreasi pantai Carita . Akibat ledakan dua orang tewas . Keduanya adalah Kobar dan Andri . Begitu dahsyatnya ledakan hingga tubuh mereka terlempar sejauh 10 meter dari tempat pusat ledakan . Selain merenggut korban jiwa ledakan juga membuat Darmin terluka parah . Belum diketahui penyebab ledakan . Namun , di sebuah rumah yang letaknya sekitar 50 meter dari lokasi ledakan polisi menemukan benda-benda yang biasa dijadikan bahan peledak , seperti belerang , potasium dan kabel . Sejak lima bulan lalu rumah itu dikontrak seorang pria bernama Sukardi yang kini menghilang . Guna mengungkap kasus ini polisi sudah memeriksa saksi-saksi termasuk Itap , pemilik rumah yang dikontrak Sukardi . Polisi juga terus menghimpun informasi tentang Sukardi , lelaki misterius yang kini buron . ( IAN/Agus Faisal Karim ) .', 'summary': 'Dua orang tewas seketika akibat ledakan dahsyat di sebuah desa di Kecamatan Carita , Pandeglang , Banten . Di dekat lokasi ledakan polisi menemukan berbagai jenis bahan peledak .'}\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas dataframes to Hugging Face datasets\n",
    "print(\"üîÑ Converting to Hugging Face Dataset format...\\n\")\n",
    "\n",
    "# Create dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(df_train[['clean_article', 'clean_summary']]),\n",
    "    'validation': Dataset.from_pandas(df_val[['clean_article', 'clean_summary']]),\n",
    "    'test': Dataset.from_pandas(df_test[['clean_article', 'clean_summary']])\n",
    "})\n",
    "\n",
    "# Rename columns\n",
    "dataset_dict = dataset_dict.rename_column('clean_article', 'article')\n",
    "dataset_dict = dataset_dict.rename_column('clean_summary', 'summary')\n",
    "\n",
    "print(\"‚úì Dataset created:\")\n",
    "print(dataset_dict)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìã Sample data:\")\n",
    "print(dataset_dict['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b07a6ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Tokenization parameters:\n",
      "  Max input length: 512\n",
      "  Max target length: 128\n",
      "\n",
      "üî§ Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70868d847a744829d83f39c4ddd53d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/9694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28de5ce260ec4527bace8de6bfeb959b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43c9885477e4c8abe014c02b77f8dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Tokenization complete:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 9694\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 549\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 549\n",
      "    })\n",
      "})\n",
      "\n",
      "üìã Sample tokenized data:\n",
      "Input IDs shape: 512\n",
      "Attention mask shape: 512\n",
      "Labels shape: 128\n",
      "\n",
      "First 20 input IDs: [2, 13665, 30385, 30470, 2231, 30468, 21136, 30472, 492, 10884, 2086, 597, 26, 4237, 4667, 901, 30468, 1351, 2969, 188]\n",
      "First 20 labels: [2, 662, 232, 6193, 11731, 1597, 10884, 9754, 26, 492, 1351, 26, 2172, 2203, 155, 30468, 21136, 30468, 5116, 30470]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization parameters\n",
    "MAX_INPUT_LENGTH = 512   # Maximum article length\n",
    "MAX_TARGET_LENGTH = 128  # Maximum summary length\n",
    "\n",
    "print(f\"üìè Tokenization parameters:\")\n",
    "print(f\"  Max input length: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"  Max target length: {MAX_TARGET_LENGTH}\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize articles and summaries for BERT2GPT model\n",
    "    \"\"\"\n",
    "    # Tokenize inputs (articles)\n",
    "    model_inputs = tokenizer(\n",
    "        examples['article'],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (summaries)\n",
    "    labels = tokenizer(\n",
    "        examples['summary'],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Replace padding token id with -100 for loss calculation\n",
    "    labels['input_ids'] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels_example]\n",
    "        for labels_example in labels['input_ids']\n",
    "    ]\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"\\nüî§ Tokenizing datasets...\")\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['article', 'summary'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Tokenization complete:\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Display tokenized sample\n",
    "print(\"\\nüìã Sample tokenized data:\")\n",
    "sample = tokenized_datasets['train'][0]\n",
    "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")\n",
    "print(f\"Labels shape: {len(sample['labels'])}\")\n",
    "print(f\"\\nFirst 20 input IDs: {sample['input_ids'][:20]}\")\n",
    "print(f\"First 20 labels: {sample['labels'][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabed00b",
   "metadata": {},
   "source": [
    "<a id=\"training-config\"></a>\n",
    "## 5. Training Configuration\n",
    "\n",
    "Setup training arguments dan metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e35b781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Metrics function defined\n"
     ]
    }
   ],
   "source": [
    "# Load ROUGE metric\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores for evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    # Extract scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean length of predictions\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "print(\"‚úì Metrics function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d41a19af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Using SAMPLE dataset - Optimized hyperparameters\n",
      "\n",
      "‚öôÔ∏è  Hyperparameters:\n",
      "  Batch size: 4\n",
      "  Gradient accumulation: 2\n",
      "  Effective batch size: 8\n",
      "\n",
      "‚úì Training Arguments configured:\n",
      "  Output directory: output\\bert2gpt_finetuned\\checkpoints\n",
      "  Effective batch size: 8\n",
      "  Number of epochs: 3\n",
      "  FP16 training: False\n",
      "  Gradient checkpointing: False\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "# ============================================================\n",
    "# Adjust these parameters based on your hardware and dataset size\n",
    "# ============================================================\n",
    "\n",
    "# Detect if using sample or full dataset\n",
    "IS_SAMPLE = config.get('use_sample', False)\n",
    "\n",
    "if IS_SAMPLE:\n",
    "    print(\"‚ö° Using SAMPLE dataset - Optimized hyperparameters\")\n",
    "    BATCH_SIZE = 4\n",
    "    GRADIENT_ACCUM_STEPS = 2\n",
    "    LEARNING_RATE = 5e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 100\n",
    "    SAVE_STEPS = 200\n",
    "    EVAL_STEPS = 200\n",
    "    LOGGING_STEPS = 50\n",
    "else:\n",
    "    print(\"üìä Using FULL dataset - Production hyperparameters\")\n",
    "    BATCH_SIZE = 8\n",
    "    GRADIENT_ACCUM_STEPS = 4\n",
    "    LEARNING_RATE = 5e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 500\n",
    "    SAVE_STEPS = 500\n",
    "    EVAL_STEPS = 500\n",
    "    LOGGING_STEPS = 100\n",
    "\n",
    "WEIGHT_DECAY = 0.01      # Weight decay\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Hyperparameters:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUM_STEPS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM_STEPS}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR / \"checkpoints\"),\n",
    "    eval_strategy=\"steps\", \n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    predict_with_generate=True,  # Use generate for evaluation\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    save_total_limit=3,  # Only keep 3 best checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    seed=SEED,\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True if not IS_SAMPLE else False,\n",
    "    optim=\"adamw_torch\",\n",
    "    # Performance optimization\n",
    "    dataloader_num_workers=0,  # 0 for Windows, 2-4 for Linux\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training Arguments configured:\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM_STEPS}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  FP16 training: {training_args.fp16}\")\n",
    "print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
    "print(f\"  Device: {training_args.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "997aecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data collator created\n"
     ]
    }
   ],
   "source": [
    "# Data collator for seq2seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19d0e5",
   "metadata": {},
   "source": [
    "<a id=\"fine-tuning\"></a>\n",
    "## 6. Fine-tuning\n",
    "\n",
    "Fine-tune model dengan Seq2SeqTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50545551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Trainer initialized\n",
      "  Training samples: 9694\n",
      "  Validation samples: 549\n",
      "  Test samples: 549\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")\n",
    "print(f\"  Training samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"  Validation samples: {len(tokenized_datasets['validation'])}\")\n",
    "print(f\"  Test samples: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 3, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting fine-tuning...\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='3636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  94/3636 20:53 < 13:24:28, 0.07 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting fine-tuning...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training complete!\\n\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(\"\\nüìä Training Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d743d",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 7. Evaluation\n",
    "\n",
    "Evaluate model pada test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üîç Evaluating on test set...\\n\")\n",
    "\n",
    "test_results = trainer.evaluate(\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    max_length=MAX_TARGET_LENGTH,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\\n\")\n",
    "print(\"üìä Test Set Results:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in test_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save test metrics\n",
    "trainer.log_metrics(\"test\", test_results)\n",
    "trainer.save_metrics(\"test\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47191e83",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>\n",
    "## 8. Inference & Testing\n",
    "\n",
    "Test model dengan beberapa contoh artikel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article_text, num_beams=10, min_length=20, max_length=80):\n",
    "    \"\"\"\n",
    "    Generate summary for a given article using the fine-tuned model\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(article_text, return_tensors='pt', max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        min_length=min_length,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # Decode summary\n",
    "    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "print(\"‚úì Summary generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dengan beberapa contoh dari test set\n",
    "NUM_EXAMPLES = 5\n",
    "\n",
    "print(f\"üìù Testing model with {NUM_EXAMPLES} examples from test set:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    # Get example\n",
    "    example = dataset_dict['test'][i]\n",
    "    article = example['article']\n",
    "    reference_summary = example['summary']\n",
    "    \n",
    "    # Generate summary\n",
    "    generated_summary = generate_summary(article)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüîπ Example {i+1}:\")\n",
    "    print(f\"\\nArticle (first 200 chars):\\n{article[:200]}...\\n\")\n",
    "    print(f\"Reference Summary:\\n{reference_summary}\\n\")\n",
    "    print(f\"Generated Summary:\\n{generated_summary}\\n\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd059c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing: input artikel sendiri\n",
    "print(\"üéØ Interactive Testing\\n\")\n",
    "print(\"Masukkan artikel Anda di bawah ini (atau gunakan contoh yang disediakan):\\n\")\n",
    "\n",
    "# Contoh artikel\n",
    "SAMPLE_ARTICLE = \"\"\"\n",
    "Jakarta - Presiden Joko Widodo (Jokowi) mengumumkan kebijakan baru terkait pengembangan \n",
    "infrastruktur digital di Indonesia. Pemerintah akan mengalokasikan dana triliunan rupiah \n",
    "untuk mempercepat pembangunan jaringan internet di daerah terpencil. Langkah ini diharapkan \n",
    "dapat mengurangi kesenjangan digital antara kota besar dan daerah pedalaman. Menteri \n",
    "Komunikasi dan Informatika menyatakan bahwa program ini akan dimulai tahun depan dengan \n",
    "target mencakup 10.000 desa dalam tahap pertama.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Uncomment baris di bawah untuk input manual\n",
    "# ARTICLE_TO_SUMMARIZE = input(\"Artikel: \")\n",
    "\n",
    "# Atau gunakan contoh\n",
    "ARTICLE_TO_SUMMARIZE = SAMPLE_ARTICLE\n",
    "\n",
    "if ARTICLE_TO_SUMMARIZE.strip():\n",
    "    print(f\"\\nüìÑ Original Article:\\n{ARTICLE_TO_SUMMARIZE}\\n\")\n",
    "    \n",
    "    # Generate summary\n",
    "    print(\"ü§ñ Generating summary...\\n\")\n",
    "    summary = generate_summary(ARTICLE_TO_SUMMARIZE)\n",
    "    \n",
    "    print(f\"üìù Generated Summary:\\n{summary}\\n\")\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"  Article length: {len(ARTICLE_TO_SUMMARIZE.split())} words\")\n",
    "    print(f\"  Summary length: {len(summary.split())} words\")\n",
    "    print(f\"  Compression ratio: {len(summary.split()) / len(ARTICLE_TO_SUMMARIZE.split()) * 100:.1f}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No article provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4f32b",
   "metadata": {},
   "source": [
    "<a id=\"save-model\"></a>\n",
    "## 9. Save Model\n",
    "\n",
    "Save fine-tuned model untuk deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "FINAL_MODEL_DIR = OUTPUT_DIR / \"final_model\"\n",
    "FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Saving final model to {FINAL_MODEL_DIR}...\\n\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(str(FINAL_MODEL_DIR))\n",
    "tokenizer.save_pretrained(str(FINAL_MODEL_DIR))\n",
    "\n",
    "print(\"‚úì Model saved\")\n",
    "print(\"‚úì Tokenizer saved\")\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    \"model_checkpoint\": MODEL_CHECKPOINT,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"max_input_length\": MAX_INPUT_LENGTH,\n",
    "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
    "    \"train_samples\": len(tokenized_datasets['train']),\n",
    "    \"val_samples\": len(tokenized_datasets['validation']),\n",
    "    \"test_samples\": len(tokenized_datasets['test']),\n",
    "    \"test_metrics\": test_results\n",
    "}\n",
    "\n",
    "with open(FINAL_MODEL_DIR / \"training_info.json\", 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"‚úì Training info saved\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üéâ MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Model location: {FINAL_MODEL_DIR}\")\n",
    "print(\"\\nüí° To load the model later:\")\n",
    "print(f\"\\nfrom transformers import BertTokenizer, EncoderDecoderModel\")\n",
    "print(f\"\\ntokenizer = BertTokenizer.from_pretrained('{FINAL_MODEL_DIR}')\")\n",
    "print(f\"tokenizer.bos_token = tokenizer.cls_token\")\n",
    "print(f\"tokenizer.eos_token = tokenizer.sep_token\")\n",
    "print(f\"model = EncoderDecoderModel.from_pretrained('{FINAL_MODEL_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf1f84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Training Summary\n",
    "\n",
    "### ‚úÖ Completed Steps:\n",
    "1. ‚úì Loaded preprocessed Liputan6 dataset\n",
    "2. ‚úì Loaded cahya/bert2gpt-indonesian-summarization model\n",
    "3. ‚úì Tokenized dataset for BERT2GPT\n",
    "4. ‚úì Fine-tuned model with Seq2SeqTrainer\n",
    "5. ‚úì Evaluated on test set with ROUGE metrics\n",
    "6. ‚úì Tested inference with sample articles\n",
    "7. ‚úì Saved fine-tuned model for deployment\n",
    "\n",
    "### üìà Model Performance:\n",
    "Check the **test_results.json** file in the output directory for detailed ROUGE scores.\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Deploy model untuk production\n",
    "2. Integrate dengan aplikasi web/API\n",
    "3. A/B testing dengan baseline model\n",
    "4. Continuous fine-tuning dengan data baru\n",
    "\n",
    "### üìö References:\n",
    "- Model: https://huggingface.co/cahya/bert2gpt-indonesian-summarization\n",
    "- Dataset: Liputan6 Indonesian Summarization\n",
    "- Framework: Hugging Face Transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pipenv) - election-rnn",
   "language": "python",
   "name": "presidential-election-sentiment-analysis-rnn-pipenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
